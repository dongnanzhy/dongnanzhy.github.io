<!DOCTYPE html>
<html>
  <head><meta name="generator" content="Hexo 3.8.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="Yan&#39;s git io">
  <meta name="keyword" content="YAN&#39;s BLOG">
  
    <link rel="shortcut icon" href="/css/images/logo.png">
  
  <title>
    
      Bayesian Methods EM | YAN&#39;s BLOG
    
  </title>
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/tomorrow.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/plugins/gitment.css">
  <script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdn.bootcss.com/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>
  <script src="/js/qrious.js"></script>
<script src="/js/gitment.js"></script>
  
  
    <!-- MathJax support START -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- MathJax support END -->
  


</head>
<div class="wechat-share">
  <img src="/css/images/logo.png">
</div>

  <body>
    <header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>YAN's BLOG</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/tags/" class="item-link">Tags</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/project/" class="item-link">Projects</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/about/" class="item-link">About</a>
          
        </li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/tags/" class="menu-link">Tags</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/project/" class="menu-link">Projects</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/about/" class="menu-link">About</a>
            
          </li>
        
      </ul>
    </div>
  </div>
</header>

    <div id="article-banner">
  <h2>Bayesian Methods EM</h2>
  <p class="post-date">2018-12-31</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body flex-box">
  <!-- Article START -->
  <article class="post-article">
    <section class="markdown-content"><h1 id="Expectation-maximization-algorithm"><a href="#Expectation-maximization-algorithm" class="headerlink" title="Expectation-maximization algorithm"></a>Expectation-maximization algorithm</h1><p>In this assignment, we will derive and implement formulas for Gaussian Mixture Model — one of the most commonly used methods for performing soft clustering of the data. </p>
<h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h3><p>We will need <figure class="highlight plain"><figcaption><span>```scikit-learn```, ```matplotlib``` libraries for this assignment</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import numpy as np</span><br><span class="line">from numpy.linalg import slogdet, det, solve</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import time</span><br><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">from grader import Grader</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure></p>
<h3 id="Grading"><a href="#Grading" class="headerlink" title="Grading"></a>Grading</h3><p>We will create a grader instance below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to the platform only after running submitting function in the last part of this assignment. If you want to make a partial submission, you can run that cell anytime you want.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grader = Grader()</span><br></pre></td></tr></table></figure>
<h2 id="Implementing-EM-for-GMM"><a href="#Implementing-EM-for-GMM" class="headerlink" title="Implementing EM for GMM"></a>Implementing EM for GMM</h2><p>For debugging we will use samples from gaussian mixture model with unknown mean, variance and priors. We also added inital values of parameters for grading purposes.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">samples = np.load(<span class="string">'samples.npz'</span>)</span><br><span class="line">X = samples[<span class="string">'data'</span>]</span><br><span class="line">pi0 = samples[<span class="string">'pi0'</span>]</span><br><span class="line">mu0 = samples[<span class="string">'mu0'</span>]</span><br><span class="line">sigma0 = samples[<span class="string">'sigma0'</span>]</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=<span class="string">'grey'</span>, s=<span class="number">30</span>)</span><br><span class="line">plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(pi0)</span><br><span class="line">print(mu0)</span><br><span class="line">print(sigma0)</span><br></pre></td></tr></table></figure>
<p><img src="/images/bayesian_methods/em/output_8_0.png" alt="png"></p>
<pre><code>[0.3451814  0.6066179  0.04820071]
[[-0.71336192  0.90635089]
 [ 0.76623673  0.82605407]
 [-1.32368279 -1.75244452]]
[[[ 1.00490413  1.89980228]
  [ 1.89980228  4.18354574]]

 [[ 1.96867815  0.78415336]
  [ 0.78415336  1.83319942]]

 [[ 0.19316335 -0.11648642]
  [-0.11648642  1.98395967]]]
</code></pre><h3 id="Reminder"><a href="#Reminder" class="headerlink" title="Reminder"></a>Reminder</h3><p>Remember, that EM algorithm is a coordinate descent optimization of variational lower bound $\mathcal{L}(\theta, q) = \int q(T) \log\frac{P(X, T|\theta)}{q(T)}dT\to \max$.</p>
<p><b>E-step</b>:<br><br>$\mathcal{L}(\theta, q) \to \max\limits_{q} \Leftrightarrow \mathcal{KL} [q(T) \,|\, p(T|X, \theta)] \to \min \limits_{q\in Q} \Rightarrow q(T) = p(T|X, \theta)$<br><br><b>M-step</b>:<br><br>$\mathcal{L}(\theta, q) \to \max\limits_{\theta} \Leftrightarrow \mathbb{E}<em>{q(T)}\log p(X,T | \theta) \to \max\limits</em>{\theta}$</p>
<p>For GMM, $\theta$ is a set of parameters that consists of mean vectors $\mu_c$, covariance matrices $\Sigma_c$ and priors $\pi_c$ for each component.</p>
<p>Latent variables $T$ are indices of components to which each data point is assigned. $T_i$ (cluster index for object $i$) is a binary vector with only one active bit in position corresponding to the true component. For example, if we have $C=3$ components and object $i$ lies in first component, $T_i = [1, 0, 0]$.</p>
<p>The joint distribution can be written as follows: $p(T, X \mid \theta) =  \prod\limits_{i=1}^N p(T_i, X_i \mid \theta) = \prod\limits_{i=1}^N \prod\limits_{c=1}^C [\pi_c \mathcal{N}(X_i \mid \mu_c, \Sigma_c)]^{T_{ic}}$.</p>
<h3 id="E-step"><a href="#E-step" class="headerlink" title="E-step"></a>E-step</h3><p>In this step we need to estimate the posterior distribution over the latent variables with fixed values of parameters: $q(T) = p(T|X, \theta)$. We will assume that $T_i$ (cluster index for object $i$) is a binary vector with only one ‘1’ in position corresponding to the true component. To do so we need to compute $\gamma_{ic} = P(T_{ic} = 1 \mid X, \theta)$. Note that $\sum\limits_{c=1}^C\gamma_{ic}=1$.</p>
<p><b>Important trick 1:</b> It is important to avoid numerical errors. At some point you will have to compute the formula of the following form: $\frac{e^{x_i}}{\sum_j e^{x_j}}$. When you compute exponents of large numbers, you get huge numerical errors (some numbers will simply become infinity). You can avoid this by dividing numerator and denominator by $e^{\max(x)}$: $\frac{e^{x_i-\max(x)}}{\sum_j e^{x_j - \max(x)}}$. After this transformation maximum value in the denominator will be equal to one. All other terms will contribute smaller values. This trick is called log-sum-exp. So, to compute desired formula you first subtract maximum value from each component in vector $X$ and then compute everything else as before.</p>
<p><b>Important trick 2:</b> You will probably need to compute formula of the form $A^{-1}x$ at some point. You would normally inverse $A$ and then multiply it by $x$. A bit faster and more numerically accurate way to do this is to solve the equation $Ay = x$. Its solution is $y=A^{-1}x$, but the equation $Ay = x$ can be solved by Gaussian elimination procedure. You can use <figure class="highlight plain"><figcaption><span>for this.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;b&gt;Other usefull functions: &lt;/b&gt; &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.slogdet.html&quot;&gt;```slogdet```&lt;/a&gt; and &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.det.html#numpy.linalg.det&quot;&gt;```det```&lt;/a&gt;</span><br><span class="line"></span><br><span class="line">&lt;b&gt;Task 1:&lt;/b&gt; Implement E-step for GMM using template below.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">from scipy.stats import multivariate_normal</span><br><span class="line">def E_step(X, pi, mu, sigma):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Performs E-step on GMM model</span><br><span class="line">    Each input is numpy array:</span><br><span class="line">    X: (N x d), data points</span><br><span class="line">    pi: (C), mixture component weights </span><br><span class="line">    mu: (C x d), mixture component means</span><br><span class="line">    sigma: (C x d x d), mixture component covariance matrices</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    gamma: (N x C), probabilities of clusters for objects</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    N = X.shape[0] # number of objects</span><br><span class="line">    C = pi.shape[0] # number of clusters</span><br><span class="line">    d = mu.shape[1] # dimension of each object</span><br><span class="line">    gamma = np.zeros((N, C)) # distribution q(T)</span><br><span class="line"></span><br><span class="line">    ### YOUR CODE HERE</span><br><span class="line">    pX_given_t = np.zeros((N, C))</span><br><span class="line">    for c in range(C):</span><br><span class="line">        model = multivariate_normal(mean=mu[c, :], cov=sigma[c, :])</span><br><span class="line">        pX_given_t[:, c] = model.pdf(X)</span><br><span class="line">    </span><br><span class="line">    pX_given_t *= pi</span><br><span class="line">    gamma = pX_given_t / np.sum(pX_given_t, axis=1, keepdims=True)</span><br><span class="line">    </span><br><span class="line">    return gamma</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gamma = E_step(X, pi0, mu0, sigma0)</span><br><span class="line">print(gamma.shape)</span><br><span class="line">grader.submit_e_step(gamma)</span><br></pre></td></tr></table></figure>
<pre><code>(280, 3)
Current answer for task Task 1 (E-step) is: 0.5337178741081263
</code></pre><h3 id="M-step"><a href="#M-step" class="headerlink" title="M-step"></a>M-step</h3><p>In M-step we need to maximize $\mathbb{E}<em>{q(T)}\log p(X,T | \theta)$ with respect to $\theta$. In our model this means that we need to find optimal values of $\pi$, $\mu$, $\Sigma$. To do so, you need to compute the derivatives and<br>set them to zero. You should start by deriving formulas for $\mu$ as it is the easiest part. Then move on to $\Sigma$. Here it is crucial to optimize function w.r.t. to $\Lambda = \Sigma^{-1}$ and then inverse obtained result. Finaly, to compute $\pi$, you will need <a href="https://www3.nd.edu/~jstiver/FIN360/Constrained%20Optimization.pdf" target="_blank" rel="noopener">Lagrange Multipliers technique</a> to satisfy constraint $\sum\limits</em>{i=1}^{n}\pi_i = 1$.</p>
<p><br><br><b>Important note:</b> You will need to compute derivatives of scalars with respect to matrices. To refresh this technique from previous courses, see <a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener"> wiki article</a> about it . Main formulas of matrix derivatives can be found in <a href="http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf" target="_blank" rel="noopener">Chapter 2 of The Matrix Cookbook</a>. For example, there you may find that $\frac{\partial}{\partial A}\log |A| = A^{-T}$.</p>
<p><b>Task 2:</b> Implement M-step for GMM using template below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">M_step</span><span class="params">(X, gamma)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs M-step on GMM model</span></span><br><span class="line"><span class="string">    Each input is numpy array:</span></span><br><span class="line"><span class="string">    X: (N x d), data points</span></span><br><span class="line"><span class="string">    gamma: (N x C), distribution q(T)  </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    pi: (C)</span></span><br><span class="line"><span class="string">    mu: (C x d)</span></span><br><span class="line"><span class="string">    sigma: (C x d x d)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N = X.shape[<span class="number">0</span>] <span class="comment"># number of objects</span></span><br><span class="line">    C = gamma.shape[<span class="number">1</span>] <span class="comment"># number of clusters</span></span><br><span class="line">    d = X.shape[<span class="number">1</span>] <span class="comment"># dimension of each object</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE</span></span><br><span class="line">    pi = np.zeros(C)</span><br><span class="line">    mu = np.zeros((C, d))</span><br><span class="line">    sigma = np.zeros((C, d, d))</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(C):</span><br><span class="line">        p_posterior_t = np.sum(gamma[:, c])</span><br><span class="line">        mu[c, :] = np.sum(gamma[:, c].reshape(N,<span class="number">1</span>) * X, axis=<span class="number">0</span>) / p_posterior_t</span><br><span class="line">        sigma[c, :] = np.sum([gamma[i,c] * np.outer(X[i,:] - mu[c,:], X[i,:] - mu[c,:]) <span class="keyword">for</span> i <span class="keyword">in</span> range(N)], axis=<span class="number">0</span>) / p_posterior_t</span><br><span class="line">        pi[c] = p_posterior_t / N</span><br><span class="line">    <span class="keyword">return</span> pi, mu, sigma</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gamma = E_step(X, pi0, mu0, sigma0)</span><br><span class="line">pi, mu, sigma = M_step(X, gamma)</span><br><span class="line">grader.submit_m_step(pi, mu, sigma)</span><br></pre></td></tr></table></figure>
<pre><code>Current answer for task Task 2 (M-step: mu) is: 2.899391882050384
Current answer for task Task 2 (M-step: sigma) is: 5.9771052168975265
Current answer for task Task 2 (M-step: pi) is: 0.5507624459218775
</code></pre><h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>Finally, we need some function to track convergence. We will use variational lower bound $\mathcal{L}$ for this purpose. We will stop our EM iterations when $\mathcal{L}$ will saturate. Usually, you will need only about 10-20 iterations to converge. It is also useful to check that this function never decreases during training. If it does, you have a bug in your code.</p>
<p><b>Task 3:</b> Implement a function that will compute $\mathcal{L}$ using template below.</p>
<p>$$\mathcal{L} = \sum_{n=1}^{N} \sum_{k=1}^{K} \mathbb{E}[z_{n, k}] (\log \pi_k + \log \mathcal{N}(x_n | \mu_k, \sigma_k)) - \sum_{n=1}^{N} \sum_{k=1}^{K} \mathbb{E}[z_{n, k}] \log \mathbb{E}[z_{n, k}]$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_vlb</span><span class="params">(X, pi, mu, sigma, gamma)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Each input is numpy array:</span></span><br><span class="line"><span class="string">    X: (N x d), data points</span></span><br><span class="line"><span class="string">    gamma: (N x C), distribution q(T)  </span></span><br><span class="line"><span class="string">    pi: (C)</span></span><br><span class="line"><span class="string">    mu: (C x d)</span></span><br><span class="line"><span class="string">    sigma: (C x d x d)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns value of variational lower bound</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N = X.shape[<span class="number">0</span>] <span class="comment"># number of objects</span></span><br><span class="line">    C = gamma.shape[<span class="number">1</span>] <span class="comment"># number of clusters</span></span><br><span class="line">    d = X.shape[<span class="number">1</span>] <span class="comment"># dimension of each object</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    small = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(C):</span><br><span class="line">        model = multivariate_normal(mu[c], sigma[c], allow_singular=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">            loss += gamma[n, c]*(np.log(pi[c] + small) + model.logpdf(X[n, :]) - np.log(gamma[n,c] + <span class="number">0.1</span>*small))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pi, mu, sigma = pi0, mu0, sigma0</span><br><span class="line">gamma = E_step(X, pi, mu, sigma)</span><br><span class="line">pi, mu, sigma = M_step(X, gamma)</span><br><span class="line">loss = compute_vlb(X, pi, mu, sigma, gamma)</span><br><span class="line">grader.submit_VLB(loss)</span><br></pre></td></tr></table></figure>
<pre><code>Current answer for task Task 3 (VLB) is: -1213.973464306017
</code></pre><h3 id="Bringing-it-all-together"><a href="#Bringing-it-all-together" class="headerlink" title="Bringing it all together"></a>Bringing it all together</h3><p>Now that we have E step, M step and VLB, we can implement training loop. We will start at random values of $\pi$, $\mu$ and $\Sigma$, train until $\mathcal{L}$ stops changing and return the resulting points. We also know that EM algorithm sometimes stops at local optima. To avoid this we should restart algorithm multiple times from different starting positions. Each training trial should stop either when maximum number of iterations is reached or when relative improvement is smaller than given tolerance ($|\frac{\mathcal{L}<em>i-\mathcal{L}</em>{i-1}}{\mathcal{L}_{i-1}}| \le \text{rtol}$).</p>
<p>Remember, that values of $\pi$ that you generate must be non-negative and sum up to 1. Also, $\Sigma$ matrices must be symmetric and positive semi-definite. If you don’t know how to generate those matrices, you can use $\Sigma=I$ as initialization.</p>
<p>You will also sometimes get numerical errors because of component collapsing. The easiest way to deal with this problems is to simply restart the procedure.</p>
<p><b>Task 4:</b> Implement training procedure</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_EM</span><span class="params">(X, C, rtol=<span class="number">1e-3</span>, max_iter=<span class="number">100</span>, restarts=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Starts with random initialization *restarts* times</span></span><br><span class="line"><span class="string">    Runs optimization until saturation with *rtol* reached</span></span><br><span class="line"><span class="string">    or *max_iter* iterations were made.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    X: (N, d), data points</span></span><br><span class="line"><span class="string">    C: int, number of clusters</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    N = X.shape[<span class="number">0</span>] <span class="comment"># number of objects</span></span><br><span class="line">    d = X.shape[<span class="number">1</span>] <span class="comment"># dimension of each object</span></span><br><span class="line">    best_loss = <span class="number">-999999</span></span><br><span class="line">    best_pi = <span class="keyword">None</span></span><br><span class="line">    best_mu = <span class="keyword">None</span></span><br><span class="line">    best_sigma = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    mu = np.zeros((C,d))</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(restarts):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment">### YOUR CODE HERE</span></span><br><span class="line">            pi = np.array([<span class="number">1.0</span>/C]*C,dtype=np.float32)</span><br><span class="line">            <span class="comment"># pi = np.array([0.35,0.35,0.3])</span></span><br><span class="line">            <span class="comment">#mu = np.random.rand(C, d)</span></span><br><span class="line">            mu[<span class="number">0</span>,:] = np.array([<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">            mu[<span class="number">1</span>,:] = np.array([<span class="number">1</span>,<span class="number">6</span>])</span><br><span class="line">            mu[<span class="number">2</span>,:] = np.array([<span class="number">7</span>,<span class="number">4</span>])</span><br><span class="line">            <span class="comment">#sigma_ = np.random.rand(C, d, d)</span></span><br><span class="line">            <span class="comment">#sigma = np.array([np.dot(A, A.T) for A in sigma_])</span></span><br><span class="line">            sigma = np.array([np.identity(d)] * C)</span><br><span class="line">            prev_loss = <span class="keyword">None</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iter):</span><br><span class="line">                gamma = E_step(X, pi, mu, sigma)</span><br><span class="line">                pi, mu, sigma = M_step(X, gamma)</span><br><span class="line">                pi = pi / np.sum(pi)</span><br><span class="line">                loss = compute_vlb(X, pi, mu, sigma, gamma)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> math.isnan(loss) <span class="keyword">and</span> loss &gt; best_loss:</span><br><span class="line">                    best_loss = loss</span><br><span class="line">                    best_mu = mu</span><br><span class="line">                    best_pi = pi</span><br><span class="line">                    best_sigma = sigma</span><br><span class="line">                <span class="comment">#print("Iteration &#123;&#125;, loss: &#123;&#125;".format(i, loss))</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> prev_loss <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    diff = np.abs(loss - prev_loss)</span><br><span class="line">                    <span class="keyword">if</span> diff &lt; rtol:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                prev_loss = loss</span><br><span class="line">        <span class="keyword">except</span> np.linalg.LinAlgError:</span><br><span class="line">            print(<span class="string">"Singular matrix: components collapsed"</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_loss, best_pi, best_mu, best_sigma</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_loss, best_pi, best_mu, best_sigma = train_EM(X, <span class="number">3</span>)</span><br><span class="line">grader.submit_EM(best_loss)</span><br></pre></td></tr></table></figure>
<pre><code>Current answer for task Task 4 (EM) is: -1063.811767605055
</code></pre><p>If you implemented all the steps correctly, your algorithm should converge in about 20 iterations. Let’s plot the clusters to see it. We will assign a cluster label as the most probable cluster index. This can be found using matrix $\gamma$ computed on last E-step. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gamma = E_step(X, best_pi, best_mu, best_sigma)</span><br><span class="line">labels = gamma.argmax(<span class="number">1</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=labels, s=<span class="number">30</span>)</span><br><span class="line">plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/bayesian_methods/em/output_29_0.png" alt="png"></p>
<h3 id="Authorization-amp-Submission"><a href="#Authorization-amp-Submission" class="headerlink" title="Authorization &amp; Submission"></a>Authorization &amp; Submission</h3><p>To submit assignment parts to Cousera platform, please, enter your e-mail and your token into variables below. You can generate the token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">STUDENT_EMAIL = <span class="string">''</span></span><br><span class="line">STUDENT_TOKEN = <span class="string">''</span></span><br><span class="line">grader.status()</span><br></pre></td></tr></table></figure>
<pre><code>You want to submit these numbers:
Task Task 1 (E-step): 0.5337178741081263
Task Task 2 (M-step: mu): 2.899391882050384
Task Task 2 (M-step: sigma): 5.9771052168975265
Task Task 2 (M-step: pi): 0.5507624459218775
Task Task 3 (VLB): -1213.973464306017
Task Task 4 (EM): -1063.811767605055
</code></pre><p>If you want to submit these answers, run cell below</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)</span><br></pre></td></tr></table></figure>
<pre><code>You used an invalid email or your token may have expired. Please make sure you have entered all fields correctly. Try generating a new token if the issue still persists.
</code></pre></section>
    <!-- Tags START -->
    
      <div class="tags">
        <span>Tags:</span>
        
  <a href="/tags#bayesian-methods">
    <span class="tag-code">bayesian-methods</span>
  </a>

  <a href="/tags#EM">
    <span class="tag-code">EM</span>
  </a>

      </div>
    
    <!-- Tags END -->
    <!-- NAV START -->
    
  <div class="nav-container">
    <!-- reverse left and right to put prev and next in a more logic postition -->
    
      <a class="nav-left" href="/2018/12/31/pandas-cheetsheet/">
        <span class="nav-arrow">← </span>
        
          Pandas Cheetsheet
        
      </a>
    
    
      <a class="nav-right" href="/2018/12/31/bayesian-methods-mcmc/">
        
          Bayesian Methods MCMC
        
        <span class="nav-arrow"> →</span>
      </a>
    
  </div>

    <!-- NAV END -->
    
      <!-- Gitment START -->
      <div id="comments"></div>
      <!-- Gitment END -->
    
  </article>
  <!-- Article END -->
  <!-- Catalog START -->
  
    <aside class="catalog-container">
  <div class="toc-main">
    <strong class="toc-title">Catalog</strong>
    
      <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#Expectation-maximization-algorithm"><span class="toc-nav-text">Expectation-maximization algorithm</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Installation"><span class="toc-nav-text">Installation</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Grading"><span class="toc-nav-text">Grading</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Implementing-EM-for-GMM"><span class="toc-nav-text">Implementing EM for GMM</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Reminder"><span class="toc-nav-text">Reminder</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#E-step"><span class="toc-nav-text">E-step</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#M-step"><span class="toc-nav-text">M-step</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Loss-function"><span class="toc-nav-text">Loss function</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Bringing-it-all-together"><span class="toc-nav-text">Bringing it all together</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Authorization-amp-Submission"><span class="toc-nav-text">Authorization &amp; Submission</span></a></li></ol></li></ol>
    
  </div>
</aside>
  
  <!-- Catalog END -->
</main>

<script>
  (function () {
    var url = 'http://yoursite.com/2018/12/31/bayesian-methods-em/';
    var banner = ''
    if (banner !== '' && banner !== 'undefined' && banner !== 'null') {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

     // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', 'http://file.muyutech.com/error-img.png')
      $(this).css({
        'cursor': 'default'
      })
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      if (src !== 'http://file.muyutech.com/error-img.png') {
        var imageW = $(this).width()
        var imageH = $(this).height()
        
        var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
        zoom = zoom < 1 ? 1 : zoom
        zoom = zoom > 2 ? 2 : zoom
        var transY = (($(window).height() - imageH) / 2).toFixed(2)

        $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
        $('.image-view-wrap').addClass('wrap-active')
        $('.image-view-wrap img').css({
          'width': `${imageW}`,
          'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
        })
        $('html').css('overflow', 'hidden')

        $('.image-view-wrap').on('click', function() {
          $(this).remove()
          $('html').attr('style', '')
        })
      }
    })

    // qrcode
    var qr = new QRious({
      element: document.getElementById('share-qrcode'),
      value: document.location.href
    });

    // gitment
    var gitmentConfig = "dongnanzhy";
    if (gitmentConfig !== 'undefined') {
      var gitment = new Gitment({
        id: "Bayesian Methods EM",
        owner: "dongnanzhy",
        repo: "dongnanzhy.github.io",
        oauth: {
          client_id: "6e8efba4b92de298d180",
          client_secret: "ef25328fb6ac8348ad6921d892776be451db3639"
        },
        theme: {
          render(state, instance) {
            const container = document.createElement('div')
            container.lang = "en-US"
            container.className = 'gitment-container gitment-root-container'
            container.appendChild(instance.renderHeader(state, instance))
            container.appendChild(instance.renderEditor(state, instance))
            container.appendChild(instance.renderComments(state, instance))
            container.appendChild(instance.renderFooter(state, instance))
            return container;
          }
        }
      })
      gitment.render(document.getElementById('comments'))
    }
  })();
</script>

<script>
  var disqus_shortname = '';
  
  var disqus_url = 'http://yoursite.com/2018/12/31/bayesian-methods-em/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//go.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    &copy; 2019 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a>
  </p>
</footer>

<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine === 'false') {
        figure.find('.gutter').hide();
      }
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->

<script src="/js/script.js"></script>
  </body>
</html>